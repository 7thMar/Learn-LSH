{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ffht**\n",
    "- [Python 2.7](https://github.com/FALCONN-LIB/FFHT)\n",
    "- [Python 3.x](https://github.com/ahazxm/FFHT)\n",
    "```sh\n",
    "git clone https://github.com/ahazxm/FFHT.git\n",
    "cd FFHT\n",
    "python setup.py install\n",
    "python example.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "from scipy.spatial import distance\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "import sys\n",
    "import ffht\n",
    "from hashlib import md5\n",
    "import timeit\n",
    "import math\n",
    "\n",
    "mbyte = 1048576\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "norm_l1 = Normalizer(norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "def hybrid_dist(x, y, d_max, verbose=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x1, y1: hamming 距离向量\n",
    "        x2, y2: 欧式距离向量\n",
    "        verbose: 显示详细内容\n",
    "    \"\"\"\n",
    "    hd = distance.hamming(x[0], y[0])\n",
    "    ed = distance.euclidean(x[1], y[1]) / d_max\n",
    "    hybrid_d = 0.5 * hd + 0.5 * ed\n",
    "    \n",
    "    if verbose is True:\n",
    "        print('hamming dist = {:.4f}, euclidean dist = {:.4f}'.format(hd, ed))\n",
    "        print('hybrid dist = {}'.format(hybrid_d))\n",
    "        \n",
    "    return hybrid_d\n",
    "\n",
    "def gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Gram-schmidt正交化\n",
    "        将所矩阵正交化\n",
    "    \"\"\"\n",
    "    Q=np.zeros_like(A)\n",
    "    cnt = 0\n",
    "    for a in A.T:\n",
    "        u = np.copy(a)\n",
    "        for i in range(0, cnt):\n",
    "            u -= np.dot(np.dot(Q[:, i].T, a), Q[:, i]) # 减去待求向量在以求向量上的投影\n",
    "        e = u / np.linalg.norm(u)  # 归一化\n",
    "        Q[:, cnt] = e\n",
    "        cnt += 1\n",
    "    R = np.dot(Q.T, A)\n",
    "    return Q\n",
    "\n",
    "def generate_md5(H):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        md5 编码\n",
    "    \"\"\"\n",
    "    hmd5 = md5()\n",
    "    hmd5.update(str(H).encode(encoding='utf-8'))\n",
    "    return hmd5.hexdigest()\n",
    "\n",
    "def transform_hybrid(P1, P2, d):\n",
    "    return np.append(P1, P2).reshape(2, -1).T.reshape(-1, 2, d)\n",
    "\n",
    "def feature_hashing(P, target_d, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        特征哈希\n",
    "    Args:\n",
    "        P: 点\n",
    "        target_d: 降维度  \n",
    "    \"\"\"\n",
    "\n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState()\n",
    "\n",
    "    d = P.shape[1]\n",
    "\n",
    "    zero_idxs = rand.randint(0, target_d, d)\n",
    "    signs = rand.choice([-1, 1], d)\n",
    "    S = np.zeros((target_d, d))\n",
    "    for i, idx in enumerate(zero_idxs):\n",
    "        S[idx][i] = signs[i]\n",
    "    \n",
    "    return np.array([S.dot(p) for p in P])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 查询到最近邻的概率\n",
    "$$\n",
    "pr = \\frac{N_{find}}{N}\n",
    "$$\n",
    "2. 准确率(现更改为错误率 error_ratio)\n",
    "$$\n",
    "E = \\frac{1}{k}\\sum^k_{i=1}\\frac{||q-N(q)_i||}{||q-I(q)_i||}\n",
    "$$\n",
    "    - $N(q)_i$ 真实的$i$近邻\n",
    "    - $I(q)_i$ 查询的$i$近邻\n",
    "3. 召回率\n",
    "4. 候选集数量(现改成 selectivity, 候选数量/数据集大小)\n",
    "5. 哈希时间\n",
    "6. 查询时间(最近邻)\n",
    "7. 空间消耗由于不同方法存储结构不同，放至函数体外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(P, query_func, args, test, dataset_title, exact_nearest_neighbor, exact_k_near_neighbors, verbose=False):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        评估\n",
    "            1. 查询到最近邻的概率\n",
    "            2. 准确率(现更改为错误率 error_ratio)\n",
    "            3. 召回率\n",
    "            4. 候选集数量(现改成 selectivity, 候选数量/数据集大小)\n",
    "            5. 哈希时间\n",
    "            6. 查询时间(最近邻)\n",
    "            7. 空间消耗由于不同方法存储结构不同，放至函数体外\n",
    "    Args:\n",
    "        P: 数据集\n",
    "        query_func: 查询函数\n",
    "        args: 查询函数所需参数\n",
    "        test: 测试集\n",
    "        dataset_title: \n",
    "        exact_nearest_neighbor: 精确的最近邻\n",
    "        exact_k_near_neighbors: 精确的k-近邻\n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    K = 50 # k近邻\n",
    "    correct = 0\n",
    "    hash_times = 0.0\n",
    "    total_times = 0.0\n",
    "    n_test = 0\n",
    "    recall = 0.0\n",
    "    error_ratio = 0.0\n",
    "    selectivity = 0.0\n",
    "    \n",
    "    for i, q in enumerate(test):\n",
    "        \n",
    "        t1 = timeit.default_timer()\n",
    "        candidates = query_func(args, q)\n",
    "        t2 = timeit.default_timer()\n",
    "        \n",
    "        if len(candidates) < K: # 排除异常点\n",
    "            continue\n",
    "        \n",
    "        hash_times += (t2 - t1 + 0.0) # 5.\n",
    "        selectivity += len(candidates) / n # 4.\n",
    "        \n",
    "        t1 = timeit.default_timer()\n",
    "        candidates_dist = np.array([hybrid_dist(q, P[idx], dmax[dataset_title]) for idx in candidates])\n",
    "        nearest_neighbor_idx = candidates[np.argsort(candidates_dist)[0]]        \n",
    "        t2 = timeit.default_timer()\n",
    "        \n",
    "        total_times += (t2 - t1 + 0.0) # 6.\n",
    "        \n",
    "        if nearest_neighbor_idx == exact_nearest_neighbor[i]:\n",
    "            correct += 1\n",
    "\n",
    "        k_near_neighbors = candidates[np.argsort(candidates_dist)[:K]]\n",
    "        n_correct_k = len(np.intersect1d(exact_k_near_neighbors[i], k_near_neighbors))\n",
    "        \n",
    "        recall += n_correct_k / K # 3 召回率\n",
    "        error_ratio += np.array([hybrid_dist(q, P[exact_k_near_neighbors[i][idx]], dmax[dataset_title]) / \n",
    "                 hybrid_dist(q, P[k_near_neighbors[idx]], dmax[dataset_title]) for idx in range(K)]).sum() / K # 2. 错误率        \n",
    "        \n",
    "        n_test += 1 # 有效测试点数量\n",
    "        \n",
    "    pr = correct / n_test # 1\n",
    "    error_ratio = error_ratio / n_test # 2\n",
    "    recall = recall / n_test # 3\n",
    "    selectivity = selectivity / n_test # 4\n",
    "    hash_times = hash_times / n_test # 5\n",
    "    total_times = (total_times + hash_times) / n_test # 6\n",
    "    \n",
    "    if verbose is True:\n",
    "        print('The probability of find the nearest neighbor is {:.4f}'.format(pr))\n",
    "        print('Error ratio: {:.4f}, Recall: {:.4f}, Selectivity: {:.4f}'.format(error_ratio, \n",
    "                                                                                recall, \n",
    "                                                                                selectivity))\n",
    "        print('Hash time: {:.4f}, total time: {:.4f}'.format(hash_times, total_times))\n",
    "\n",
    "    return pr, error_ratio, recall, selectivity, hash_times, total_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[synset](https://github.com/7thMar/synset) **synset-1e4d128-train/test.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_nearest_neighbor = dict()\n",
    "dic_k_near_neighbors = dict()\n",
    "dmax = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('../synset/OPRCP/synset-1e4d128-train.csv').reshape([-1, 2, 128])\n",
    "test = np.loadtxt('../synset/OPRCP/synset-1e4d128-test.csv').reshape([-1, 2, 128])\n",
    "dataset_title = '1e4d128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax[dataset_title] = euclidean_distances(train[:, 1, :]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储 **最近邻** 与 **K近邻**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nearest_neighbor = []\n",
    "k_near_neighbors = []\n",
    "for q in test[:100]:\n",
    "    ds = np.array([hybrid_dist(q, p, dmax[dataset_title]) for p in train])\n",
    "    order_ds_args = np.argsort(ds)\n",
    "    nearest_neighbor.append(order_ds_args[0])\n",
    "    k_near_neighbors.append(order_ds_args[:K])\n",
    "    \n",
    "dic_nearest_neighbor[dataset_title] = nearest_neighbor\n",
    "dic_k_near_neighbors[dataset_title] = k_near_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对比函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPRCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 标准 OPR，CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_polytope(d, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        生成正轴型 d 维, 2 * d 个\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    A = np.random.normal(0, 1, (d, d))\n",
    "    A = norm_l2.fit_transform(gram_schmidt(A))\n",
    "    A = np.append(A, -A).reshape(-1, d)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP(P, k, L, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "        center: 点集均值\n",
    "        e： 正轴型坐标\n",
    "        As: 旋转向量\n",
    "        seeds: 排列种子（用于进行相同的打乱重排过程）\n",
    "        auxiliary_vector: 辅助向量\n",
    "        buckets: 桶\n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    print('k = {}, L = {}'.format(k, L))\n",
    "    print('Every segament length is {}'.format(sd))\n",
    "\n",
    "    buckets = []\n",
    "\n",
    "    seeds = np.random.choice(np.arange(k * L * d), L, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    As = np.array([norm_l2.fit_transform(A) for A in np.random.normal(0, 1, (L, d, d))]) # 生成辅助旋转矩阵\n",
    "\n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "        A = As[i]\n",
    "\n",
    "        # 旋转\n",
    "        rotate = norm_l2.fit_transform([A.dot(p) for p in P2])\n",
    "        # 取离的最近的点的索引\n",
    "        H2 = rotate.dot(e.T).argmax(axis=1)\n",
    "\n",
    "        for idx, p in enumerate(P):\n",
    "            np.random.seed(seeds[i])\n",
    "            permutate_p = np.random.permutation(p[0])\n",
    "\n",
    "            H1 = -((auxiliary_vector * permutate_p).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "\n",
    "            H = np.append(H1, H2[idx]) # H=(H1, H2)\n",
    "            bi = generate_md5(H)\n",
    "            if bi not in bucket:\n",
    "                bucket[bi] = [idx]\n",
    "            else:\n",
    "                bucket[bi].append(idx)\n",
    "\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    return center, e, As, seeds, auxiliary_vector, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单层存储空间缩小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    center, e, As, seeds, auxiliary_vector, buckets = args[0]\n",
    "    m = args[1]\n",
    "    k, L = args[2]\n",
    "    \n",
    "    q1 = q[0].copy()\n",
    "    d = q1.shape[0]\n",
    "    \n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    sd = int(d / k)\n",
    "\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "\n",
    "        np.random.seed(seeds[i])\n",
    "        q1_ = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * q1_).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "        \n",
    "        rotate = norm_l2.fit_transform([As[i].dot(q2)])[0]\n",
    "        H2 = rotate.dot(e.T).argsort()[::-1][:m]\n",
    "        \n",
    "        for h2 in H2:\n",
    "            H = np.append(H1, h2)\n",
    "            bi = generate_md5(H)\n",
    "            if bi in buckets[i]:\n",
    "                result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L = 34\n",
    "m = 2\n",
    "oprcp = OPRCP(train, k=k, L=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, e, As, seeds, auxiliary_vector, buckets = oprcp\n",
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, OPRCP_query, [oprcp, m, (k, L)],\n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPRCP version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加速旋转过程（库函数替代循环过程）\n",
    "2. 构建阶段进行多探测（即生成的哈希值对应最近的 **m** 个索引值）\n",
    "3. 查询阶段不使用多探测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP2(P, k, L, m, random_state=None):\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    buckets = []\n",
    "\n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState()\n",
    "    \n",
    "    # OPR param\n",
    "    seeds = rand.choice(np.arange(k * L * d), L, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "\n",
    "    # CP param\n",
    "    e = generate_cross_polytope(d, random_state)[:d] # 生成正轴型向量 2 * d 个\n",
    "    As = np.array([norm_l2.fit_transform(A) for A in np.random.normal(0, 1, (L, d, d))]) # 生成辅助旋转矩阵\n",
    "    \n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "        A = As[i]\n",
    "        \n",
    "        # 库函数替代循环从而加速旋转过程\n",
    "        rotate = norm_l2.fit_transform(A.dot(P2.T).T)\n",
    "        # 对距离进行排序（对应最优方法，未完全实现）\n",
    "        H2 = np.abs(rotate.dot(e.T)).argsort(axis=1)\n",
    "        for idx, p in enumerate(P):\n",
    "            np.random.seed(seeds[i])\n",
    "            permutate_p = np.random.permutation(p[0])\n",
    "\n",
    "            H1 = -((auxiliary_vector * permutate_p).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "\n",
    "            # 构建阶段进行多探测，取前m近的索引为哈希值\n",
    "            for h2 in H2[idx][::-1][:m]:\n",
    "                bi = generate_md5([H1, h2])\n",
    "                if bi not in bucket:\n",
    "                    bucket[bi] = [idx]\n",
    "                else:\n",
    "                    bucket[bi].append(idx)\n",
    "\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    return center, e, As, seeds, auxiliary_vector, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP2_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, e, As, seeds, auxiliary_vector, buckets = args[0]\n",
    "    m, k, L = args[1]\n",
    "\n",
    "\n",
    "    q1 = q[0].copy()\n",
    "    d = q1.shape[0]\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    sd = int(d / k)\n",
    "\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(L):\n",
    "        np.random.seed(seeds[i])\n",
    "        permutate_q = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * permutate_q).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "\n",
    "        rotate = norm_l2.fit_transform([As[i].dot(q2)])[0]\n",
    "        H2 = np.abs(rotate.dot(e.T)).argmax()\n",
    "        bi = generate_md5([H1, H2])\n",
    "        if bi in buckets[i]:\n",
    "            result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L = 20\n",
    "m = 3\n",
    "oprcp = OPRCP2(train, k, L, m, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in oprcp[-1]:\n",
    "    for key, b in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(b)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, OPRCP2_query, [oprcp, (m, k, L)],\n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPRCP version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **fast hadamard** 实现伪旋转替代真旋转（加速旋转过程）\n",
    "2. 构建阶段进行多探测（即生成的哈希值对应最近的 **m** 个索引值）\n",
    "3. 查询阶段不使用多探测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_polytope(d, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        生成正轴型 d 维, 2 * d 个\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    A = np.random.normal(0, 1, (d, d))\n",
    "    A = norm_l2.fit_transform(gram_schmidt(A))\n",
    "    A = np.append(A, -A).reshape(-1, d)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP3(P, k, L, m, num_rotations=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    print('k = {}, L = {}'.format(k, L))\n",
    "    print('Every segament length is {}'.format(sd))\n",
    "\n",
    "    buckets = []\n",
    "    \n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    \n",
    "    seeds = rand.choice(np.arange(k * L * d), L, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    e = e[:d]\n",
    "    Ds = rand.choice([-1, 1], (L, num_rotations, d))\n",
    "\n",
    "    idxs = np.arange(n)\n",
    "\n",
    "    \n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "        rotate = []\n",
    "        \n",
    "        # fast hadamard 实现伪旋转替代真旋转\n",
    "        for p in P2:\n",
    "            for j in range(num_rotations):\n",
    "                p = p * Ds[i][j]\n",
    "                ffht.fht(p)\n",
    "            rotate.append(p)\n",
    "        rotate = np.array(rotate)\n",
    "        rotate = norm_l2.fit_transform(rotate)\n",
    "        H2 = np.abs(rotate.dot(e.T)).argsort(axis=1)\n",
    "\n",
    "        for idx, p in enumerate(P):\n",
    "            np.random.seed(seeds[i])\n",
    "            php = np.random.permutation(p[0])\n",
    "\n",
    "            H1 = -((auxiliary_vector * php).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "           \n",
    "            for h2 in H2[idx][::-1][:m]:\n",
    "                H = np.append(H1, h2)\n",
    "\n",
    "                bi = generate_md5(H)\n",
    "                if bi not in bucket:\n",
    "                    bucket[bi] = [idx]\n",
    "                else:\n",
    "                    bucket[bi].append(idx)\n",
    "\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    return center, e, Ds, seeds, auxiliary_vector, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP3_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, e, Ds, seeds, auxiliary_vector, buckets = args[0]\n",
    "    m = args[1]\n",
    "    k, L = args[2]\n",
    "    num_rotations = args[3]\n",
    "\n",
    "    q1 = q[0].copy()\n",
    "    d = q1.shape[0]\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    \n",
    "    sd = int(d / k)\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(L):\n",
    "        np.random.seed(seeds[i])\n",
    "        q1_ = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * q1_).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "\n",
    "        q3 = q2.copy()\n",
    "        for j in range(num_rotations):\n",
    "            q3 = q3 * Ds[i][j]\n",
    "            ffht.fht(q3)\n",
    "        rotate = norm_l2.fit_transform([q3])[0]\n",
    "        H2 = np.abs(rotate.dot(e.T)).argmax()\n",
    "        H = np.append(H1, H2)\n",
    "        bi = generate_md5(H)\n",
    "        if bi in buckets[i]:\n",
    "            result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 2\n",
    "L = 10\n",
    "m = 3\n",
    "num_rotations = 3\n",
    "oprcp = OPRCP3(train, k=k, L=L, m=m, num_rotations=num_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, e, Ds, seeds, auxiliary_vector, buckets = oprcp\n",
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, OPRCP3_query, [oprcp, m, (k, L), num_rotations],\n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPRCP version 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **fast hadamard** 实现伪旋转替代真旋转（加速旋转过程）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP4(P, k, L, num_rotations=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    print('k = {}, L = {}'.format(k, L))\n",
    "    print('Every segament length is {}'.format(sd))\n",
    "\n",
    "    buckets = []\n",
    "    \n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    \n",
    "    seeds = rand.choice(np.arange(k * L * d), L, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    Ds = rand.choice([-1, 1], (L, num_rotations, d))\n",
    "\n",
    "\n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "\n",
    "        rotate = []\n",
    "        for p in P2:\n",
    "            for j in range(num_rotations):\n",
    "                p = p * Ds[i][j]\n",
    "                ffht.fht(p)\n",
    "            rotate.append(p)\n",
    "        rotate = np.array(rotate)\n",
    "        rotate = norm_l2.fit_transform(rotate) # 加速\n",
    "        H2 = np.abs(rotate.dot(e[:d].T)).argmax(axis=1)\n",
    "\n",
    "        for idx, p in enumerate(P):\n",
    "\n",
    "            np.random.seed(seeds[i])\n",
    "            php = np.random.permutation(p[0])\n",
    "\n",
    "            H1 = -((auxiliary_vector * php).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "\n",
    "            H = np.append(H1, H2[idx])\n",
    "            # print(H)\n",
    "            bi = generate_md5(H)\n",
    "            if bi not in bucket:\n",
    "                bucket[bi] = [idx]\n",
    "            else:\n",
    "                bucket[bi].append(idx)\n",
    "       #  print(i)\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    return center, e, Ds, seeds, auxiliary_vector, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP4_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, e, Ds, seeds, auxiliary_vector, buckets = args[0]\n",
    "    m = args[1]\n",
    "    k, L = args[2]\n",
    "    num_rotations = args[3]\n",
    "    \n",
    "    q1 = q[0].copy()\n",
    "    d = q1.shape[0]\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    sd = int(d / k)\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "\n",
    "        np.random.seed(seeds[i])\n",
    "        q1_ = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * q1_).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "        \n",
    "        q3 = q2.copy()\n",
    "        for j in range(num_rotations):\n",
    "            q3 = q3 * Ds[i][j]\n",
    "            ffht.fht(q3)\n",
    "        rotate = norm_l2.fit_transform([q3])[0]\n",
    "        H2 = np.abs(rotate.dot(e[:d].T)).argsort()[::-1][:m]\n",
    "        \n",
    "        for h2 in H2:\n",
    "            H = np.append(H1, h2)\n",
    "            bi = generate_md5(H)\n",
    "            if bi in buckets[i]:\n",
    "                result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 2\n",
    "L = 10\n",
    "num_rotations = 3\n",
    "oprcp = OPRCP4(train, k=k, L=L, num_rotations=num_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, e, Ds, seeds, auxiliary_vector, buckets = oprcp\n",
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, OPRCP4_query, [oprcp, m, (k, L), num_rotations],\n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPRCP version 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加速旋转过程（库函数替代循环过程）\n",
    "2. 构建阶段进行多探测（即生成的哈希值对应最近的 **m** 个索引值）\n",
    "3. 查询阶段使用多探测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP5(P, k, L, m, random_state=None):\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    buckets = []\n",
    "\n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState()\n",
    "    \n",
    "    # OPR param\n",
    "    seeds = rand.choice(np.arange(k * L * d), L, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "\n",
    "    # CP param\n",
    "    e = generate_cross_polytope(d, random_state)[:d] # 生成正轴型向量 2 * d 个\n",
    "    As = np.array([norm_l2.fit_transform(A) for A in np.random.normal(0, 1, (L, d, d))]) # 生成辅助旋转矩阵\n",
    "    \n",
    "    for i in range(L):\n",
    "        bucket = dict()\n",
    "        A = As[i]\n",
    "        \n",
    "        # 库函数替代循环从而加速旋转过程\n",
    "        rotate = norm_l2.fit_transform(A.dot(P2.T).T)\n",
    "        # 对距离进行排序（对应最优方法，未完全实现）\n",
    "        H2 = np.abs(rotate.dot(e.T)).argsort(axis=1)\n",
    "        for idx, p in enumerate(P):\n",
    "            np.random.seed(seeds[i])\n",
    "            permutate_p = np.random.permutation(p[0])\n",
    "\n",
    "            H1 = -((auxiliary_vector * permutate_p).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "\n",
    "            # 构建阶段进行多探测，取前m近的索引为哈希值\n",
    "            for h2 in H2[idx][::-1][:m]:\n",
    "                bi = generate_md5([H1, h2])\n",
    "                if bi not in bucket:\n",
    "                    bucket[bi] = [idx]\n",
    "                else:\n",
    "                    bucket[bi].append(idx)\n",
    "\n",
    "        buckets.append(bucket)\n",
    "\n",
    "    return center, e, As, seeds, auxiliary_vector, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPRCP5_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, e, As, seeds, auxiliary_vector, buckets = args[0]\n",
    "    m, k, L = args[1]\n",
    "\n",
    "\n",
    "    q1 = q[0].copy()\n",
    "    d = q1.shape[0]\n",
    "    C = math.ceil(d / k + L) + 2\n",
    "    sd = int(d / k)\n",
    "\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(L):\n",
    "        np.random.seed(seeds[i])\n",
    "        permutate_q = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * permutate_q).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "\n",
    "        rotate = norm_l2.fit_transform([As[i].dot(q2)])[0]\n",
    "\n",
    "    \n",
    "        # 查询阶段同样使用多探测\n",
    "        H2 = np.abs(rotate.dot(e.T)).argsort()[::-1][:m]\n",
    "        for h in H2:\n",
    "            bi = generate_md5([H1, h])\n",
    "            if bi in buckets[i]:\n",
    "                result.append(buckets[i][bi])\n",
    "    \n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L = 12\n",
    "m = 3\n",
    "oprcp = OPRCP5(train, k, L, m, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in oprcp[-1]:\n",
    "    for key, b in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(b)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, OPRCP5_query, [oprcp, (m, k, L)],\n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINCP version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINCP_version1(P, k, L, num_rotations=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Min-hash + cross polytope\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: \n",
    "        L: 桶数\n",
    "    Returns:\n",
    "        center: mean\n",
    "        e: cross-polytope\n",
    "        Ds: fast hadamard \n",
    "        H_mat: min hash\n",
    "        buckets\n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "\n",
    "    P1 = P[\n",
    "        :, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    buckets = []\n",
    "    for i in range(L):\n",
    "        buckets.append(dict())\n",
    "    \n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    \n",
    "    num_hash = k * L\n",
    "\n",
    "    # 1. 生成 hash 函数\n",
    "    h_mat = np.arange(1, d + 1)\n",
    "    H_mat = np.array([rand.permutation(h_mat) for _ in range(num_hash)])\n",
    "\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    e = e[:d]\n",
    "    Ds = rand.choice([-1, 1], (L, num_rotations, d))\n",
    "\n",
    "    H2 = []\n",
    "    for i in range(L):\n",
    "        rotate = []\n",
    "        for p in P2:\n",
    "            for j in range(num_rotations):\n",
    "                p = p * Ds[i][j]\n",
    "                ffht.fht(p)\n",
    "            rotate.append(p)\n",
    "        rotate = norm_l2.fit_transform(np.array(rotate)) # 加速\n",
    "        H2.append(np.abs(rotate.dot(e.T)).argmax(axis=1))\n",
    "\n",
    "    for idx, p in enumerate(P1):\n",
    "        H1 = (p * H_mat).astype(np.float)\n",
    "        H1[H1 == 0] = np.inf\n",
    "        H1 = H1.min(axis=1).reshape(L, k)\n",
    "        for i in range(L):\n",
    "            H = np.append(H1[i], H2[i][idx])\n",
    "            bi = generate_md5(H)\n",
    "            if bi not in buckets[i]:\n",
    "                buckets[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets[i][bi].append(idx)\n",
    "\n",
    "    return center, e, Ds, H_mat, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINCP_version1_query(args, q):\n",
    "\n",
    "    center, e, Ds, H_mat, buckets = args[0]\n",
    "    m = args[1]\n",
    "    k, L = args[2]\n",
    "    num_rotations = args[3]\n",
    "\n",
    "    q1 = q[0].copy()\n",
    "    \n",
    "    d = q1.shape[0]\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    H1 = (q1 * H_mat).astype(np.float)\n",
    "    H1[H1 == 0] = np.inf\n",
    "    H1 = H1.min(axis=1).reshape(L, k)\n",
    "\n",
    "    for i in range(L):\n",
    "        q3 = q2.copy()\n",
    "        for j in range(num_rotations):\n",
    "            q3 = q3 * Ds[i][j]\n",
    "            ffht.fht(q3)\n",
    "        rotate = norm_l2.fit_transform([q3])[0]\n",
    "        H2 = np.abs(rotate.dot(e.T)).argsort()[::-1][:m]\n",
    "\n",
    "        for h2 in H2:\n",
    "            H = np.append(H1[i], h2)\n",
    "            bi = generate_md5(H)\n",
    "            if bi in buckets[i]:\n",
    "                result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L = 13\n",
    "num_rotations = 3\n",
    "lsh = MINCP_version1(train, k=k, L=L, num_rotations=num_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, e, Ds, H_mat, buckets = lsh\n",
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, MINCP_version1_query, [lsh, m, (k, L), num_rotations], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINCP version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINCP_version2(P, k, L, num_rotations=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    buckets = []\n",
    "    for i in range(L):\n",
    "        buckets.append(dict())\n",
    "    \n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    \n",
    "    num_hash = k * L\n",
    "\n",
    "    # 1. 生成 hash 函数\n",
    "    h_mat = np.arange(1, d + 1)\n",
    "    H_mat = np.array([rand.permutation(h_mat) for _ in range(num_hash)])\n",
    "\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    Ds = rand.choice([-1, 1], (L, num_rotations, d))\n",
    "\n",
    "    H2 = []\n",
    "    for i in range(L):\n",
    "        rotate = []\n",
    "        for p in P2:\n",
    "            for j in range(num_rotations):\n",
    "                p = p * Ds[i][j]\n",
    "                ffht.fht(p)\n",
    "            rotate.append(p)\n",
    "        rotate = norm_l2.fit_transform(np.array(rotate)) # 加速\n",
    "        H2.append(np.abs(rotate.dot(e[:d].T)).argmax(axis=1))\n",
    "\n",
    "    for idx, p in enumerate(P1):\n",
    "        H1 = (p * H_mat).astype(np.float)\n",
    "        H1[H1 == 0] = np.inf\n",
    "        H1 = H1.min(axis=1).reshape(L, k)\n",
    "        for i in range(L):\n",
    "            H = np.append(H1[i], H2[i][idx])\n",
    "            bi = generate_md5(H)\n",
    "            if bi not in buckets[i]:\n",
    "                buckets[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets[i][bi].append(idx)\n",
    "\n",
    "    return center, e, Ds, H_mat, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINCP_version2_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, e, Ds, H_mat, buckets = args[0]\n",
    "    m = args[1]\n",
    "    k, L = args[2]\n",
    "    num_rotations = args[3]\n",
    "\n",
    "    q1 = q[0].copy()\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    d = q1.shape[0]\n",
    "    \n",
    "    H1 = (q1 * H_mat).astype(np.float)\n",
    "    H1[H1 == 0] = np.inf\n",
    "    H1 = H1.min(axis=1).reshape(L, k)\n",
    "\n",
    "    for i in range(L):\n",
    "        q3 = q2.copy()\n",
    "        for j in range(num_rotations):\n",
    "            q3 = q3 * Ds[i][j]\n",
    "            ffht.fht(q3)\n",
    "        rotate = norm_l2.fit_transform([q3])[0]\n",
    "        H2 = np.abs(rotate.dot(e[:d].T)).argsort()[::-1][:m]\n",
    "\n",
    "        for h2 in H2:\n",
    "            H = np.append(H1[i], h2)\n",
    "            bi = generate_md5(H)\n",
    "            if bi in buckets[i]:\n",
    "                result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L = 15\n",
    "num_rotations = 3\n",
    "lsh = MINCP_version2(train, k=k, L=L, num_rotations=num_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "center, e, Ds, H_mat, buckets = lsh\n",
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, MINCP_version2_query, [lsh, m, (k, L), num_rotations], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINHASHHP(Min-Hash and Hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINHP_process(P, k1, k2, L, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        L: 桶数\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    buckets = []\n",
    "    for i in range(L):\n",
    "        buckets.append(dict())\n",
    "    \n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    \n",
    "    num_hash = k1 * L\n",
    "\n",
    "    # 1. 生成 hash 函数\n",
    "    h_mat = np.arange(1, d + 1)\n",
    "    H_mat = np.array([rand.permutation(h_mat) for _ in range(num_hash)])\n",
    "    V = np.array([norm_l2.fit_transform(V) for V in rand.normal(0, 1, (L, k2, d))])\n",
    "\n",
    "    H2 = []\n",
    "    for i in range(L):\n",
    "        H2.append(np.array(P2.dot(V[i].T) > 0, dtype=np.int))\n",
    "\n",
    "    for idx, p in enumerate(P1):\n",
    "        H1 = (p * H_mat).astype(np.float)\n",
    "        H1[H1 == 0] = np.inf\n",
    "        H1 = H1.min(axis=1).reshape(L, k1)\n",
    "        for i in range(L):\n",
    "            H = np.append(H1[i], H2[i][idx]).astype(np.int)\n",
    "\n",
    "            bi = generate_md5(H)\n",
    "            if bi not in buckets[i]:\n",
    "                buckets[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets[i][bi].append(idx)\n",
    "\n",
    "    return center, H_mat, V, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MINHP_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    center, H_mat, V, buckets = args[0]\n",
    "    k1, k2, L = args[1]\n",
    "    \n",
    "    q1 = q[0].copy()\n",
    "    # 正则化至单位球面\n",
    "    q2 = q[1].copy()\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "    result = []\n",
    "    \n",
    "    H1 = (q1 * H_mat).astype(np.float)\n",
    "    H1[H1 == 0] = np.inf\n",
    "    H1 = H1.min(axis=1).reshape(L, k1)\n",
    "    \n",
    "    for i in range(L):\n",
    "        H2 = np.array(q2.dot(V[i].T) > 0, dtype=np.int)\n",
    "\n",
    "        H = np.append(H1[i], H2).astype(np.int)\n",
    "\n",
    "        bi = generate_md5(H)\n",
    "        if bi in buckets[i]:\n",
    "            result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "\n",
    "        result = np.unique(np.concatenate(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k1 = 2\n",
    "k2 = 4\n",
    "L = 42\n",
    "lsh = MINHP_process(train, k1, k2, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center, H_mat, V, buckets = lsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, MINHP_query, [lsh, (k1, k2, L)], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLOPRCP 双层过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_polytope(d, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        生成正轴型 d 维, 2 * d 个\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        rand = np.random.RandomState(random_state)\n",
    "    else:\n",
    "        rand = np.random.RandomState()\n",
    "    A = rand.normal(0, 1, (d, d))\n",
    "    A = norm_l2.fit_transform(gram_schmidt(A))\n",
    "    A = np.append(A, -A).reshape(-1, d)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLOPRCP(P, k, L1, L2, random_state=None):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集 n * 2 * d\n",
    "        k: 分段数\n",
    "            sd: D / k 每段长度\n",
    "        L: 桶数\n",
    "        C: 防碰撞参数\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L1) + 2\n",
    "    P1 = P[:, 0].copy()\n",
    "    P2 = P[:, 1].copy()\n",
    "    center = np.mean(P2, axis=0)\n",
    "    P2 = P2 - center\n",
    "    P2 = norm_l2.fit_transform(P2)\n",
    "\n",
    "    print('k = {}, L = {}'.format(k, L1))\n",
    "    print('Every segament length is {}'.format(sd))\n",
    "\n",
    "    seeds = np.random.choice(np.arange(k * L1 * d), L1, replace=False)\n",
    "    one_segament_vector = list(range(sd, 0, -1))\n",
    "    auxiliary_vector = np.array(one_segament_vector * k)\n",
    "    e = generate_cross_polytope(d, random_state) # 生成正轴型向量 2 * d 个\n",
    "    As = np.array([norm_l2.fit_transform(A) for A in np.random.normal(0, 1, (L2, d, d))]) # 生成辅助旋转矩阵\n",
    "    idxs = np.arange(n)\n",
    "\n",
    "    buckets1 = []\n",
    "    buckets2 = []\n",
    "    for i in range(L1):\n",
    "        bucket = dict()\n",
    "        for idx, p in enumerate(P):\n",
    "            np.random.seed(seeds[i])\n",
    "            php = np.random.permutation(p[0])\n",
    "            H1 = -((auxiliary_vector * php).reshape(-1, sd).max(axis=1) - sd)\n",
    "            H1[H1 == sd] = -1\n",
    "            for j, h in enumerate(H1):\n",
    "                if h == -1:\n",
    "                    nj = j\n",
    "                    while H1[nj] == -1:\n",
    "                        nj = (1 + nj) % k\n",
    "                    H1[j] = H1[nj] + C\n",
    "    \n",
    "            bi = generate_md5(H1)\n",
    "            if bi not in bucket:\n",
    "                bucket[bi] = [idx]\n",
    "            else:\n",
    "                bucket[bi].append(idx)\n",
    "\n",
    "        buckets1.append(bucket)\n",
    "    \n",
    "    \n",
    "    idxs = np.arange(n)\n",
    "    for i in range(L2):\n",
    "        bucket = dict()\n",
    "        A = As[i]\n",
    "        rotate = norm_l2.fit_transform([A.dot(p) for p in P2]) # 加速\n",
    "        H2 = rotate.dot(e.T).argmax(axis=1)\n",
    "        \n",
    "        for i in range(e.shape[0]):\n",
    "            part = idxs[H2 == i]\n",
    "            if len(part) != 0:\n",
    "                bucket[i] = part\n",
    "\n",
    "        buckets2.append(bucket)\n",
    "        \n",
    "    result = dict()\n",
    "    result['center'] = center\n",
    "    result['e'] = e\n",
    "    result['As'] = As\n",
    "    result['P'] = transform_hybrid(P1, P2, d)\n",
    "    result['seeds'] = seeds\n",
    "    result['auxiliary'] = auxiliary_vector\n",
    "    result['buckets1'] = buckets1\n",
    "    result['buckets2'] = buckets2\n",
    "    result['k'] = k\n",
    "    result['L1'] = L1\n",
    "    result['L2'] = L2\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLOPRCP_query(args, q):\n",
    "    P = args[0]['P']\n",
    "    n = P.shape[0]\n",
    "    d = P.shape[2]\n",
    "    center = args[0]['center']\n",
    "    k = args[0]['k']\n",
    "    L1 = args[0]['L1']\n",
    "    L2 = args[0]['L2']\n",
    "    seeds = args[0]['seeds']\n",
    "    auxiliary_vector = args[0]['auxiliary']\n",
    "    e = args[0]['e']\n",
    "    sd = int(d / k)\n",
    "    C = math.ceil(d / k + L1) + 2\n",
    "    buckets1 = args[0]['buckets1']\n",
    "    buckets2 = args[0]['buckets2']\n",
    "    As = args[0]['As']\n",
    "    \n",
    "    m = args[1]\n",
    "\n",
    "    q1 = q[0]\n",
    "    q2 = q[1]\n",
    "    q2 = q2 - center\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "    # 计算hamming\n",
    "    for i in range(L1):\n",
    "        np.random.seed(seeds[i])\n",
    "        php = np.random.permutation(q1)\n",
    "        H1 = -((auxiliary_vector * php).reshape(-1, sd).max(axis=1) - sd)\n",
    "        H1[H1 == sd] = -1\n",
    "        for j, h in enumerate(H1):\n",
    "            if h == -1:\n",
    "                nj = j\n",
    "                while H1[nj] == -1:\n",
    "                    nj = (1 + nj) % k\n",
    "                H1[j] = H1[nj] + C\n",
    "\n",
    "        bi = generate_md5(H1)\n",
    "        if bi in buckets1[i]:\n",
    "            result1.append(buckets1[i][bi])\n",
    "    \n",
    "\n",
    "    for i in range(L2):\n",
    "        bucket = buckets2[i]\n",
    "        A = As[i]\n",
    "        rotate = norm_l2.fit_transform([A.dot(q2)])[0]\n",
    "        H = rotate.dot(e.T).argsort()[::-1][:m]\n",
    "        for h in H:\n",
    "            if h in bucket:\n",
    "                result2.append(bucket[h])\n",
    "\n",
    "                                \n",
    "    if len(result1) != 0 and len(result2) != 0:\n",
    "        result1 = np.unique(np.concatenate(result1))\n",
    "        result2 = np.unique(np.concatenate(result2))\n",
    "        result = np.intersect1d(result1, result2)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 4\n",
    "L1 = 10\n",
    "L2 = 5\n",
    "m = 3\n",
    "tloprcp = TLOPRCP(train, k, L1, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets1, buckets2 = tloprcp['buckets1'], tloprcp['buckets2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in buckets1:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "for bucket in buckets2:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, TLOPRCP_query, [tloprcp, m], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLMinHashE2LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLMinE2LSH(object):\n",
    "    def __init__(self, P):\n",
    "        P1 = P[:, 0].copy()\n",
    "        P2 = P[:, 1].copy()\n",
    "        self.P = P\n",
    "        self.hamming_P = P1\n",
    "        self.eculidean_P = P2\n",
    "\n",
    "        self.n_ = self.hamming_P.shape[0]\n",
    "        self.d_ = self.hamming_P.shape[1]\n",
    "\n",
    "        # 设置 r1\n",
    "        self.set_r1()\n",
    "        self.set_d1()\n",
    "    \n",
    "    def _logab(self, a, b):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            求解 log_a b\n",
    "        \"\"\"\n",
    "        return np.log(b) / np.log(a)\n",
    "\n",
    "    def _Pr(self, w, c):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            w: 段长\n",
    "            c: r1, r2, 距离\n",
    "        \"\"\"\n",
    "        a = 2 * norm.cdf(-w/c)\n",
    "        b = 2 / (np.sqrt(2 * np.pi) * w / c)\n",
    "        d = np.e ** (-((w**2) / (2 * (c ** 2))))    \n",
    "        return 1 - a - b * (1 - d)\n",
    "    \n",
    "    def _rho(self, p1, p2):\n",
    "        return (np.log(1/p1) / np.log(1/p2))\n",
    "\n",
    "    def set_r1(self, r1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 r1\n",
    "        \"\"\"\n",
    "        P = self.hamming_P\n",
    "        print('Estimate r1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if r1 is not None:\n",
    "            self.r1_ = r1\n",
    "            print('set r1 = {}'.format(r1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        r1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.minkowski(q, p, p=1) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            r1_avg.append(avg)\n",
    "        r1 = math.ceil(np.array(r1_avg).mean())\n",
    "        self.r1_ = r1\n",
    "        print('estimate r1 = {}'.format(self.r1_))\n",
    "        return r1\n",
    "\n",
    "    def set_d1(self, d1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 d1\n",
    "        \"\"\"\n",
    "        P = self.eculidean_P\n",
    "        d = P.shape[1]\n",
    "        print('Estimate d1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if d1 is not None:\n",
    "            self.d1_ = d1\n",
    "            print('set d1 = {}'.format(d1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        d1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.euclidean(q, p) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            d1_avg.append(avg)\n",
    "        d1 = np.array(d1_avg).mean()\n",
    "        \n",
    "        self.d1_ = d1\n",
    "        print('estimate d1 = {}'.format(self.d1_))\n",
    "        return d1\n",
    "\n",
    "    def draw_W(self, c=2):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            绘制图像，观察最优 w 值\n",
    "        Args:\n",
    "            c: 近似因子\n",
    "        \"\"\"\n",
    "        d1 = self.d1_\n",
    "        W = np.arange(d1 / 2, d1 * 10, d1 / 2)\n",
    "        self.c_ = c\n",
    "        self.d2_ = c * d1\n",
    "    \n",
    "        p1 = [self._Pr(w, d1) for w in W]\n",
    "        p2 = [self._Pr(w, c * d1) for w in W]\n",
    "        rho = [self._rho(p1[i], p2[i]) for i in range(len(W))]\n",
    "\n",
    "        plt.plot(W, p1, label='p1')\n",
    "        plt.plot(W, p2, label='p2')\n",
    "        plt.plot(W, rho, label='rho')\n",
    "        plt.hlines(y=1/c, xmin=0, xmax=d1 * 10, label='1/c')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('p')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def set_k(self, k1, k2):\n",
    "        self.k1_ = k1\n",
    "        self.k2_ = k2\n",
    "    def set_L(self, L1, L2):\n",
    "        self.L1_ = L1\n",
    "        self.L2_ = L2\n",
    "\n",
    "    def set_param(self, w, delta=np.e):\n",
    "        n = self.hamming_P.shape[0]\n",
    "        self.r2_ = self.r1_ * self.c_\n",
    "        self.d2_ = self.d1_ * self.c_\n",
    "        d = self.d_\n",
    "        self.delta_ = delta\n",
    "        self.w_ = w\n",
    "\n",
    "        # hamming 距离\n",
    "        p1 = 1 - self.r1_ / d\n",
    "        p2 = 1 - self.r2_ / d\n",
    "        rho = self._rho(p1, p2)\n",
    "        k1 = math.ceil(self._logab(p2, 1/n))\n",
    "        self.hamming_p1_ = p1\n",
    "        self.hamming_p2_ = p2\n",
    "        self.k1_ = k1\n",
    "        self.hamming_rho_ = rho\n",
    "        \n",
    "\n",
    "        euclidean_p1 = self._Pr(w, self.d1_)\n",
    "        euclidean_p2 = self._Pr(w, self.d2_)\n",
    "        rho = self._rho(p1, p2)\n",
    "        k2 = math.ceil(self._logab(p2, 1/n))\n",
    "        self.euclidean_p1_ = euclidean_p1\n",
    "        self.euclidean_p2_ = euclidean_p2\n",
    "        self.k2_ = k2\n",
    "        self.euclidean_rho_ = rho\n",
    "\n",
    "        if self.delta_ == np.e:\n",
    "            L1 = math.ceil(n ** self.hamming_rho_)\n",
    "            L2 = math.ceil(n ** self.euclidean_rho_)\n",
    "        else:\n",
    "            L1 = math.ceil(math.log( 1 / self.delta_) / -math.log(1 - self.hamming_p1_ ** self.k1_))  \n",
    "            L2 = math.ceil(math.log( 1 / self.delta_) / -math.log(1 - self.euclidean_p1_ ** self.k2_))  \n",
    "    \n",
    "        self.L1_ = L1\n",
    "        self.L2_ = L2\n",
    "        print('E2LSH set w = {}'.format(w))\n",
    "        print('For hamming: k = {}, p1 = {}, p2 = {}'.format(k1, self.hamming_p1_, self.hamming_p2_))\n",
    "        print('For euclidean: k = {}, p1 = {}, p2 = {}'.format(k2, self.euclidean_p1_, self.euclidean_p2_))\n",
    "        print('L1 = {}, L2 = {}'.format(L1, L2))\n",
    "\n",
    "    def generate_auxiliary_vector(self):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            生成 G, 生成 H_mat\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.d_\n",
    "        w = self.w_\n",
    "        k1 = self.k1_\n",
    "        k2 = self.k2_\n",
    "        L1 = self.L1_\n",
    "        L2 = self.L2_\n",
    "        print('Generate auxiliary vector')\n",
    "        # 生成 G\n",
    "        h_mat = np.arange(1, d + 1)\n",
    "        H_mat = np.array([np.random.permutation(h_mat) for _ in range(k1 * L1)])\n",
    "        self.H_mat_ = H_mat\n",
    "        \n",
    "        G = []\n",
    "        for i in range(L2):\n",
    "            g = []\n",
    "            for i in range(k2):\n",
    "                a = np.random.normal(0, 1, d)\n",
    "                b = np.random.uniform(0, w)\n",
    "                g.append((a, b))\n",
    "            G.append(g)\n",
    "        self.G_ = G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLMinE2LSH_process(minhash_e2lsh_param):\n",
    "    P = minhash_e2lsh_param.P\n",
    "    n = P.shape[0]\n",
    "    d = minhash_e2lsh_param.hamming_P.shape[1]\n",
    "\n",
    "    L1 = minhash_e2lsh_param.L1_\n",
    "    L2 = minhash_e2lsh_param.L2_\n",
    "    # for hamming\n",
    "    k1 = minhash_e2lsh_param.k1_\n",
    "    H_mat = minhash_e2lsh_param.H_mat_\n",
    "\n",
    "    # for euclidean\n",
    "    k2 = minhash_e2lsh_param.k2_\n",
    "    G = minhash_e2lsh_param.G_\n",
    "    w = minhash_e2lsh_param.w_\n",
    "\n",
    "    buckets1 = []\n",
    "    buckets2 = []\n",
    "    for i in range(L1):\n",
    "        buckets1.append(dict())\n",
    "    for i in range(L2):\n",
    "        buckets2.append(dict())\n",
    "\n",
    "    for idx, p in enumerate(P):\n",
    "        hamming_p = p[0]\n",
    "\n",
    "        h1 = (hamming_p * H_mat).astype(np.float)\n",
    "        h1[h1 == 0] = np.inf\n",
    "        h1 = h1.min(axis=1).reshape(L1, k1)\n",
    "        for i in range(L1):\n",
    "            bi = generate_md5(h1[i])\n",
    "            if bi not in buckets1[i]:\n",
    "                buckets1[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets1[i][bi].append(idx)\n",
    "        \n",
    "        euclidean_p = p[1]\n",
    "        for i in range(L2):\n",
    "            g = G[i]\n",
    "            g_val = np.array([math.ceil((euclidean_p.dot(h[0]) + h[1]) / w) for h in g])\n",
    "            bi = generate_md5(g_val)\n",
    "\n",
    "            if bi not in buckets2[i]:\n",
    "                buckets2[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets2[i][bi].append(idx)\n",
    "\n",
    "    return buckets1, buckets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLMinE2LSH_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集\n",
    "        args:\n",
    "            [0]: hybird_E2LSH_param\n",
    "            [1]: [buckets1, buckets2]\n",
    "        q: 查询点\n",
    "    \"\"\"\n",
    "    P_ = args[0].P\n",
    "    n = args[0].n_\n",
    "    d = args[0].d_\n",
    "\n",
    "    L1 = args[0].L1_\n",
    "    L2 = args[0].L2_\n",
    "\n",
    "    # for hamming\n",
    "    k1 = args[0].k1_\n",
    "    H_mat = args[0].H_mat_\n",
    "    # for euclidean\n",
    "    k2 = args[0].k2_\n",
    "    w = args[0].w_\n",
    "\n",
    "    G = args[0].G_\n",
    "\n",
    "    buckets1 = args[1][0]\n",
    "    buckets2 = args[1][1]\n",
    "\n",
    "    q1 = q[0]\n",
    "    q2 = q[1]\n",
    "\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "\n",
    "    h = (q1 * H_mat).astype(np.float)\n",
    "    h[h == 0] = np.inf\n",
    "    h = h.min(axis=1).reshape(L1, k1)\n",
    "    for i in range(L1):\n",
    "        bi = generate_md5(h[i])\n",
    "        if bi in buckets1[i]:\n",
    "            result1.append(buckets1[i][bi])\n",
    "\n",
    "    for i in range(L2):\n",
    "        g = G[i]\n",
    "        g_val = np.array([math.ceil((q2.dot(h[0]) + h[1]) / w) for h in g])\n",
    "        bi = generate_md5(g_val)\n",
    "        \n",
    "        if bi in buckets2[i]:\n",
    "            result2.append(buckets2[i][bi])\n",
    "\n",
    "    result = []\n",
    "    if len(result1) != 0 and len(result2) != 0:\n",
    "        result1 = np.unique(np.concatenate(result1))\n",
    "        result2 = np.unique(np.concatenate(result2))\n",
    "        result = np.intersect1d(result1, result2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tlminhashe2lsh = TLMinE2LSH(P=train)\n",
    "tlminhashe2lsh.draw_W(c=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tlminhashe2lsh.set_param(200, delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tlminhashe2lsh.set_k(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlminhashe2lsh.set_L(150, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlminhashe2lsh.set_L(100, 400)\n",
    "tlminhashe2lsh.generate_auxiliary_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlminhashe2lsh.generate_auxiliary_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "buckets = TLMinE2LSH_process(tlminhashe2lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for bu in bucket:\n",
    "        for key, b in bu.items():\n",
    "            size += sys.getsizeof(key)\n",
    "            size += sys.getsizeof(b)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, TLMinE2LSH_query, [tlminhashe2lsh, buckets], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid E2LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridE2LSH(object):\n",
    "    def __init__(self, P):\n",
    "        self.P = P.copy()\n",
    "        self.P = self.P.astype(np.float)\n",
    "        self.P[:, 1] = np.array(norm_l2.fit_transform(self.P[:, 1]), dtype=np.float)\n",
    "        # print(self.P[0][1])\n",
    "        self.hamming_P = self.P[:, 0]\n",
    "        self.eculidean_P = self.P[:, 1]\n",
    "\n",
    "        self.n_ = self.hamming_P.shape[0]\n",
    "        self.d_ = self.hamming_P.shape[1]\n",
    "\n",
    "        # 设置 r1\n",
    "        self.set_r1(self.d_)\n",
    "        self.set_d1()\n",
    "\n",
    "    def _logab(self, a, b):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            求解 log_a b\n",
    "        \"\"\"\n",
    "        return np.log(b) / np.log(a)\n",
    "\n",
    "    def _Pr(self, w, c, nm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            w: 段长\n",
    "            c: r1, r2, 距离\n",
    "        \"\"\" \n",
    "        if nm == 'l2':\n",
    "            a = 2 * norm.cdf(-w/c)\n",
    "            b = 2 / (np.sqrt(2 * np.pi) * w / c)\n",
    "            d = np.e ** (-((w**2) / (2 * (c ** 2))))    \n",
    "            return 1 - a - b * (1 - d)\n",
    "        elif nm == 'l1':\n",
    "            a = w/c\n",
    "            b = 2 * (np.arctan(a)) / np.pi\n",
    "            d = 1 / (np.pi * a)\n",
    "            e = np.log(1 + (a**2))\n",
    "            return b - d * e\n",
    "    \n",
    "    def _rho(self, p1, p2):\n",
    "        return (np.log(1/p1) / np.log(1/p2))\n",
    "\n",
    "    def set_r1(self, r1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 r1\n",
    "        \"\"\"\n",
    "        P = self.hamming_P\n",
    "        print('Estimate r1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if r1 is not None:\n",
    "            self.r1_ = r1\n",
    "            print('set r1 = {}'.format(r1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        r1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.minkowski(q, p, p=1) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            r1_avg.append(avg)\n",
    "        r1 = math.ceil(np.array(r1_avg).mean())\n",
    "        self.r1_ = r1\n",
    "        print('estimate r1 = {}'.format(self.r1_))\n",
    "        return r1\n",
    "\n",
    "    def set_d1(self, d1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 d1\n",
    "        \"\"\"\n",
    "        P = self.eculidean_P\n",
    "        d = P.shape[1]\n",
    "        print('Estimate d1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if d1 is not None:\n",
    "            self.d1_ = d1\n",
    "            print('set d1 = {}'.format(d1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        d1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.euclidean(q, p) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            d1_avg.append(avg)\n",
    "        d1 = np.array(d1_avg).mean()\n",
    "        \n",
    "        self.d1_ = d1\n",
    "        print('estimate d1 = {}'.format(self.d1_))\n",
    "        return d1\n",
    "\n",
    "    def draw_W(self, c1=4, c2=2):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            绘制图像，观察最优 w 值\n",
    "        Args:\n",
    "            c: 近似因子\n",
    "        \"\"\"\n",
    "        self.c1_ = c1\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        r1 = self.r1_\n",
    "        Wr = np.arange(r1 / 2, r1 * 10, r1)\n",
    "        self.r2_ = c1 * r1\n",
    "        \n",
    "        d1 = self.d1_\n",
    "        Wd = np.arange(d1 / 2, d1 * 10, d1 / 2)\n",
    "        self.d2_ = c2 * d1\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.title('hamming')\n",
    "        p1 = [self._Pr(w, r1, 'l1') for w in Wr]\n",
    "        p2 = [self._Pr(w, c1 * r1, 'l1') for w in Wr]\n",
    "        rho = [self._rho(p1[i], p2[i]) for i in range(len(Wr))]\n",
    "\n",
    "        plt.plot(Wr, p1, label='p1')\n",
    "        plt.plot(Wr, p2, label='p2')\n",
    "        plt.plot(Wr, rho, label='rho')\n",
    "        plt.hlines(y=1/c1, xmin=0, xmax=r1 * 10, label='1/c')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('p')\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title('euclidean')\n",
    "        p1 = [self._Pr(w, d1, 'l2') for w in Wd]\n",
    "        p2 = [self._Pr(w, c2 * d1, 'l2') for w in Wd]\n",
    "        rho = [self._rho(p1[i], p2[i]) for i in range(len(Wd))]\n",
    "\n",
    "        plt.plot(Wd, p1, label='p1')\n",
    "        plt.plot(Wd, p2, label='p2')\n",
    "        plt.plot(Wd, rho, label='rho')\n",
    "        plt.hlines(y=1/c2, xmin=0, xmax=d1 * 10, label='1/c')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('p')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def set_k1_k2(self, k1, k2):\n",
    "        self.k1_ = k1\n",
    "        self.k2_ = k2\n",
    "\n",
    "    def set_L(self, L):\n",
    "        self.L_ = L\n",
    "\n",
    "    def set_param(self, w1, w2, delta=np.e):\n",
    "        self.delta_ = delta\n",
    "        \n",
    "        n = self.n_\n",
    "\n",
    "        # hamming 距离\n",
    "        p1 = self._Pr(w1, self.r1_, 'l1')\n",
    "        p2 = self._Pr(w1, self.r2_, 'l1')\n",
    "        rho = self._rho(p1, p2)\n",
    "        k1 = math.ceil(self._logab(p2, (1 - np.sqrt(1 - 1 / n))))\n",
    "        \n",
    "        print('For Hamming')\n",
    "        print('w1 = {}'.format(w1))\n",
    "        print('p1 = {:.4f}, p2 = {:.4f}, rho = {:.4f}'.format(p1, p2, rho))\n",
    "        print('k1 = {}'.format(k1))\n",
    "        \n",
    "        self.hamming_p1_ = p1\n",
    "        self.hamming_p2_ = p2\n",
    "        self.hamming_rho_ = rho\n",
    "        \n",
    "\n",
    "        # 欧式距离\n",
    "        p1 = self._Pr(w2, self.d1_, 'l2')\n",
    "        p2 = self._Pr(w2, self.d2_, 'l2')\n",
    "        rho = self._rho(p1, p2)\n",
    "        k2 = math.ceil(self._logab(p2, (1 - np.sqrt(1 - 1/ n))))\n",
    "\n",
    "        print('For Euclidean')\n",
    "        print('w2 = {}'.format(w2))\n",
    "        print('p1 = {:.4f}, p2 = {:.4f}, rho = {:.4f}'.format(p1, p2, rho))\n",
    "        print('k2 = {}'.format(k2))\n",
    "        \n",
    "        self.euclidean_p1_ = p1\n",
    "        self.euclidean_p2_ = p2\n",
    "        self.euclidean_rho_ = rho\n",
    "        \n",
    "        self.k1_ = k1\n",
    "        self.w1_ = w1\n",
    "        self.k2_ = k2\n",
    "        self.w2_ = w2\n",
    "\n",
    "                    \n",
    "        if self.delta_ == np.e:\n",
    "            L = math.ceil((1 - np.sqrt(1 - 1/n)) ** (-(\n",
    "                self.hamming_rho_ + self.euclidean_rho_)))\n",
    "        else:\n",
    "            L = math.ceil(math.log( 1 / self.delta_) /\n",
    "                          -math.log(1 - (1 - (np.sqrt(1-1/n))) ** (self.hamming_rho_ + self.euclidean_rho_)))\n",
    "        \n",
    "        self.L_ = L\n",
    "        print('L = {}'.format(L))\n",
    "    \n",
    "    def generate_auxiliary_vector(self):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            生成 G, 生成 H_mat\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.d_\n",
    "        w1 = self.w1_\n",
    "        w2 = self.w2_\n",
    "        k1 = self.k1_\n",
    "        k2 = self.k2_\n",
    "        L = self.L_\n",
    "        print('Generate auxiliary vector: G.')\n",
    "        print('For each g in G (total {}), have {} hash functions.'.format(L, k2))\n",
    "\n",
    "        # 生成 G\n",
    "        G1 = []\n",
    "        G2 = []\n",
    "\n",
    "        for i in range(L):\n",
    "            g = []\n",
    "            for i in range(k1):\n",
    "                a = np.random.standard_cauchy(d)\n",
    "                b = np.random.uniform(0, w1)\n",
    "                g.append((a, b))\n",
    "            G1.append(np.array(g))\n",
    "\n",
    "            g = []\n",
    "            for i in range(k2):\n",
    "                a = np.random.normal(0, 1, d)\n",
    "                b = np.random.uniform(0, w2)\n",
    "                g.append((a, b))\n",
    "            G2.append(np.array(g))\n",
    "            \n",
    "        self.fp_rands_ = np.random.randint(1, 6 * (k1 + k2), (L, k1 + k2))\n",
    "            \n",
    "        self.G1_ = np.array(G1)\n",
    "        self.G2_ = np.array(G2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HybridE2LSH_process(param):\n",
    "    P = param.P\n",
    "    n = P.shape[0]\n",
    "    d = param.hamming_P.shape[1]\n",
    "\n",
    "    L = param.L_\n",
    "    # for hamming\n",
    "    k1 = param.k1_\n",
    "    w1 = param.w1_\n",
    "    \n",
    "    # for euclidean\n",
    "    k2 = param.k2_\n",
    "    w2 = param.w2_\n",
    "    \n",
    "    G1 = param.G1_\n",
    "    G2 = param.G2_\n",
    "    fp_rands = param.fp_rands_\n",
    "    buckets = []\n",
    "    C = 2 ** 32 - 5\n",
    "\n",
    "    for i in range(L):\n",
    "        buckets.append(dict())\n",
    "\n",
    "    for idx, p in enumerate(P):\n",
    "        hamming_p = p[0]\n",
    "        euclidean_p = p[1]\n",
    "\n",
    "        for i in range(L):\n",
    "            g1 = G1[i]\n",
    "            g1_val = np.array([math.ceil((hamming_p.dot(h[0]) + h[1]) / w1) for h in g1])\n",
    "\n",
    "            g2 = G2[i]\n",
    "            g2_val = np.array([math.ceil((euclidean_p.dot(h[0]) + h[1]) / w2) for h in g2])\n",
    "\n",
    "            h = np.append(g1_val, g2_val)\n",
    "\n",
    "            \n",
    "            bi = generate_md5(h)\n",
    "            \n",
    "            if bi not in buckets[i]:\n",
    "                buckets[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets[i][bi].append(idx)\n",
    "\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HybridE2LSH_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集\n",
    "        args:\n",
    "            [0]: hybird_E2LSH_param\n",
    "            [1]: buckets\n",
    "        q: 查询点\n",
    "    \"\"\"\n",
    "    P = args[0].P\n",
    "    n = args[0].n_\n",
    "    d = args[0].d_\n",
    "\n",
    "\n",
    "    L = args[0].L_\n",
    "\n",
    "    # for hamming\n",
    "    k1 = args[0].k1_\n",
    "    w1 = args[0].w1_\n",
    "\n",
    "    # for euclidean\n",
    "    k2 = args[0].k2_\n",
    "    w2 = args[0].w2_\n",
    "\n",
    "    G1 = args[0].G1_\n",
    "    G2 = args[0].G2_\n",
    "\n",
    "    buckets = args[1]\n",
    "\n",
    "    q1 = q[0]\n",
    "    q2 = q[1]\n",
    "\n",
    "    q2 = norm_l2.fit_transform([q2])[0]\n",
    "    \n",
    "    # print(q1)\n",
    "    # print(q2)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for i in range(L):\n",
    "        g1 = G1[i]\n",
    "        g1_val = np.array([math.ceil((q1.dot(h[0]) + h[1]) / w1) for h in g1])\n",
    "    \n",
    "        g2 = G2[i]\n",
    "        g2_val = np.array([math.ceil((q2.dot(h[0]) + h[1]) / w2) for h in g2])\n",
    "        \n",
    "        h = np.append(g1_val, g2_val)\n",
    "        \n",
    "            \n",
    "        bi = generate_md5(h)\n",
    "\n",
    "        if bi in buckets[i]:\n",
    "            result.append(buckets[i][bi])\n",
    "\n",
    "    if len(result) != 0:\n",
    "        result = np.unique(np.concatenate(result))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lsh = HybridE2LSH(train)\n",
    "lsh.draw_W(c1=3,c2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsh.set_param(w1=500, w2=3, delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh.set_k1_k2(7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh.set_L(500)\n",
    "lsh.generate_auxiliary_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "buckets = HybridE2LSH_process(lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for key, value in bucket.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, HybridE2LSH_query, [lsh, buckets], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Level E2LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLE2LSH(object):\n",
    "    def __init__(self, P):\n",
    "        P1 = P[:, 0].copy()\n",
    "        P2 = P[:, 1].copy()\n",
    "        d = P2.shape[1]\n",
    "        self.P = P\n",
    "        self.hamming_P = P1\n",
    "        self.eculidean_P = P2\n",
    "\n",
    "        self.n_ = self.hamming_P.shape[0]\n",
    "        self.d_ = self.hamming_P.shape[1]\n",
    "\n",
    "        # 设置 r1\n",
    "        self.set_r1()\n",
    "        self.set_d1()\n",
    "\n",
    "    def _logab(self, a, b):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            求解 log_a b\n",
    "        \"\"\"\n",
    "        return np.log(b) / np.log(a)\n",
    "\n",
    "    def _Pr(self, w, c, nm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            w: 段长\n",
    "            c: r1, r2, 距离\n",
    "        \"\"\" \n",
    "        if nm == 'l2':\n",
    "            a = 2 * norm.cdf(-w/c)\n",
    "            b = 2 / (np.sqrt(2 * np.pi) * w / c)\n",
    "            d = np.e ** (-((w**2) / (2 * (c ** 2))))    \n",
    "            return 1 - a - b * (1 - d)\n",
    "        elif nm == 'l1':\n",
    "            a = w/c\n",
    "            b = 2 * (np.arctan(a)) / np.pi\n",
    "            d = 1 / (np.pi * a)\n",
    "            e = np.log(1 + (a**2))\n",
    "            return b - d * e\n",
    "    \n",
    "    def _rho(self, p1, p2):\n",
    "        return (np.log(1/p1) / np.log(1/p2))\n",
    "\n",
    "    def set_r1(self, r1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 r1\n",
    "        \"\"\"\n",
    "        P = self.hamming_P\n",
    "        print('Estimate r1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if r1 is not None:\n",
    "            self.r1_ = r1\n",
    "            print('set r1 = {}'.format(r1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        r1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.minkowski(q, p, p=1) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            r1_avg.append(avg)\n",
    "        r1 = math.ceil(np.array(r1_avg).mean())\n",
    "        self.r1_ = r1\n",
    "        print('estimate r1 = {}'.format(self.r1_))\n",
    "        return r1\n",
    "\n",
    "    def set_d1(self, d1=None):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            获取数据集 d1\n",
    "        \"\"\"\n",
    "        P = self.eculidean_P\n",
    "        d = P.shape[1]\n",
    "        print('Estimate d1')\n",
    "        print('P: {}'.format(P.shape))\n",
    "        if d1 is not None:\n",
    "            self.d1_ = d1\n",
    "            print('set d1 = {}'.format(d1))\n",
    "            return\n",
    "\n",
    "        n = 20\n",
    "        nn = int(np.log(P.shape[0]))\n",
    "        print('sample {} points, calculate near {} neighbors.'.format(n, nn))\n",
    "        idx = np.random.choice(P.shape[0], size=n, replace=False)\n",
    "        qs = P[idx]\n",
    "        d1_avg = []\n",
    "        for q in qs:\n",
    "            dist_arr = np.array([distance.euclidean(q, p) for p in P])\n",
    "            dist_sort = np.sort(dist_arr)\n",
    "            avg = np.sort(dist_arr)[1:nn+1].mean()\n",
    "            d1_avg.append(avg)\n",
    "        d1 = np.array(d1_avg).mean()\n",
    "        \n",
    "        self.d1_ = d1\n",
    "        print('estimate d1 = {}'.format(self.d1_))\n",
    "        return d1\n",
    "\n",
    "    def draw_W(self, c1=3, c2=3):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            绘制图像，观察最优 w 值\n",
    "        Args:\n",
    "            c: 近似因子\n",
    "        \"\"\"\n",
    "        self.c1_ = c1\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        r1 = self.r1_\n",
    "        Wr = np.arange(r1 / 2, r1 * 10, r1)\n",
    "        self.r2_ = c1 * r1\n",
    "        \n",
    "        d1 = self.d1_\n",
    "        Wd = np.arange(d1 / 2, d1 * 10, d1 / 2)\n",
    "        self.d2_ = c2 * d1\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.title('hamming')\n",
    "        p1 = [self._Pr(w, r1, 'l1') for w in Wr]\n",
    "        p2 = [self._Pr(w, c1 * r1, 'l1') for w in Wr]\n",
    "        rho = [self._rho(p1[i], p2[i]) for i in range(len(Wr))]\n",
    "\n",
    "        plt.plot(Wr, p1, label='p1')\n",
    "        plt.plot(Wr, p2, label='p2')\n",
    "        plt.plot(Wr, rho, label='rho')\n",
    "        plt.hlines(y=1/c1, xmin=0, xmax=r1 * 10, label='1/c')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('p')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.title('euclidean')\n",
    "        p1 = [self._Pr(w, d1, 'l2') for w in Wd]\n",
    "        p2 = [self._Pr(w, c2 * d1, 'l2') for w in Wd]\n",
    "        rho = [self._rho(p1[i], p2[i]) for i in range(len(Wd))]\n",
    "\n",
    "        plt.plot(Wd, p1, label='p1')\n",
    "        plt.plot(Wd, p2, label='p2')\n",
    "        plt.plot(Wd, rho, label='rho')\n",
    "        plt.hlines(y=1/c2, xmin=0, xmax=d1 * 10, label='1/c')\n",
    "        plt.xlabel('w')\n",
    "        plt.ylabel('p')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def set_k1_k2(self, k1, k2):\n",
    "        self.k1_ = k1\n",
    "        self.k2_ = k2\n",
    "\n",
    "    def set_L(self, L):\n",
    "        self.L_ = L\n",
    "\n",
    "    def set_param(self, w1, w2, delta=np.e):\n",
    "        self.delta_ = delta\n",
    "\n",
    "        n = self.n_\n",
    "\n",
    "        # hamming 距离\n",
    "        p1 = self._Pr(w1, self.r1_, 'l1')\n",
    "        p2 = self._Pr(w1, self.r2_, 'l1')\n",
    "        rho = self._rho(p1, p2)\n",
    "        k1 = math.ceil(self._logab(p2, 1/n))\n",
    "        # k1 = math.ceil(self._logab(p2, (1 - np.sqrt(1 - 1 / n))))\n",
    "\n",
    "        print('For Hamming')\n",
    "        print('w1 = {}'.format(w1))\n",
    "        print('p1 = {:.4f}, p2 = {:.4f}, rho = {:.4f}'.format(p1, p2, rho))\n",
    "        print('k1 = {}'.format(k1))\n",
    "        \n",
    "        self.hamming_p1_ = p1\n",
    "        self.hamming_p2_ = p2\n",
    "        self.hamming_rho_ = rho\n",
    "        \n",
    "\n",
    "        # 欧式距离\n",
    "        p1 = self._Pr(w2, self.d1_, 'l2')\n",
    "        p2 = self._Pr(w2, self.d2_, 'l2')\n",
    "        rho = self._rho(p1, p2)\n",
    "        k2 = math.ceil(self._logab(p2, 1/n))\n",
    "        # k2 = math.ceil(self._logab(p2, (1 - np.sqrt(1 - 1/ n))))\n",
    "\n",
    "        print('For Euclidean')\n",
    "        print('w2 = {}'.format(w2))\n",
    "        print('p1 = {:.4f}, p2 = {:.4f}, rho = {:.4f}'.format(p1, p2, rho))\n",
    "        print('k2 = {}'.format(k2))\n",
    "        \n",
    "        self.euclidean_p1_ = p1\n",
    "        self.euclidean_p2_ = p2\n",
    "        self.euclidean_rho_ = rho\n",
    "        \n",
    "        self.k1_ = k1\n",
    "        self.w1_ = w1\n",
    "        self.k2_ = k2\n",
    "        self.w2_ = w2\n",
    "\n",
    "        if self.delta_ == np.e:\n",
    "            L1 = math.ceil(n ** self.hamming_rho_)\n",
    "            L2 = math.ceil(n ** self.euclidean_rho_)\n",
    "        else:\n",
    "            L1 = math.ceil(math.log( 1 / self.delta_) / -math.log(1 - self.hamming_p1_ ** self.k1_))  \n",
    "            L2 = math.ceil(math.log( 1 / self.delta_) / -math.log(1 - self.euclidean_p1_ ** self.k2_))  \n",
    "    \n",
    "        self.L1_ = L1\n",
    "        self.L2_ = L2\n",
    "        print('L1 = {}, L2 = {}'.format(L1, L2))\n",
    "    \n",
    "    def generate_auxiliary_vector(self):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            生成 G, 生成 H_mat\n",
    "        \"\"\"\n",
    "\n",
    "        d = self.d_\n",
    "        w1 = self.w1_\n",
    "        w2 = self.w2_\n",
    "        k1 = self.k1_\n",
    "        k2 = self.k2_\n",
    "        L1 = self.L1_\n",
    "        L2 = self.L2_\n",
    "        # print('Generate auxiliary vector: G.')\n",
    "        # print('For each g in G (total {}), have {} hash functions.'.format(L, k2))\n",
    "\n",
    "        # 生成 G\n",
    "        G1 = []\n",
    "        G2 = []\n",
    "\n",
    "        for i in range(L1):\n",
    "            g = []\n",
    "            for i in range(k1):\n",
    "                a = np.random.standard_cauchy(d)\n",
    "                b = np.random.uniform(0, w1)\n",
    "                g.append((a, b))\n",
    "            G1.append(np.array(g))\n",
    "\n",
    "        for i in range(L2):\n",
    "            g = []\n",
    "            for i in range(k2):\n",
    "                a = np.random.normal(0, 1, d)\n",
    "                b = np.random.uniform(0, w2)\n",
    "                g.append((a, b))\n",
    "            G2.append(np.array(g))\n",
    "                \n",
    "        self.G1_ = np.array(G1)\n",
    "        self.G2_ = np.array(G2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLE2LSH_process(param):\n",
    "    P = param.P\n",
    "    n = P.shape[0]\n",
    "    d = param.hamming_P.shape[1]\n",
    "\n",
    "    # for hamming\n",
    "    L1 = param.L1_\n",
    "    k1 = param.k1_\n",
    "    w1 = param.w1_\n",
    "    \n",
    "    # for euclidean\n",
    "    L2 = param.L2_\n",
    "    k2 = param.k2_\n",
    "    w2 = param.w2_\n",
    "    \n",
    "    G1 = param.G1_\n",
    "    G2 = param.G2_\n",
    "\n",
    "    buckets1 = []\n",
    "    buckets2 = []\n",
    "\n",
    "    for i in range(L1):\n",
    "        buckets1.append(dict())\n",
    "\n",
    "    for i in range(L2):\n",
    "        buckets2.append(dict())\n",
    "\n",
    "    for idx, p in enumerate(P):\n",
    "        hamming_p = p[0]\n",
    "        for i in range(L1):\n",
    "            g = G1[i]\n",
    "            g_val = np.array([math.ceil((hamming_p.dot(h[0]) + h[1]) / w1) for h in g])\n",
    "            bi = generate_md5(g_val)\n",
    "            if bi not in buckets1[i]:\n",
    "                buckets1[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets1[i][bi].append(idx)\n",
    "\n",
    "        euclidean_p = p[1]\n",
    "        for i in range(L2):\n",
    "            g = G2[i]\n",
    "            g_val = np.array([math.ceil((euclidean_p.dot(h[0]) + h[1]) / w2) for h in g])\n",
    "            bi = generate_md5(g_val)\n",
    "            if bi not in buckets2[i]:\n",
    "                buckets2[i][bi] = [idx]\n",
    "            else:\n",
    "                buckets2[i][bi].append(idx)\n",
    "\n",
    "\n",
    "    return [buckets1, buckets2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查询函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLE2LSH_query(args, q):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "    Args:\n",
    "        P: 点集\n",
    "        args:\n",
    "            [0]: hybird_E2LSH_param\n",
    "            [1]: [buckets1, buckets2]\n",
    "        q: 查询点\n",
    "    \"\"\"\n",
    "    P_ = args[0].P\n",
    "    n = args[0].n_\n",
    "    d = args[0].d_\n",
    "\n",
    "    L1 = args[0].L1_\n",
    "    L2 = args[0].L2_\n",
    "\n",
    "    # for hamming\n",
    "    k1 = args[0].k1_\n",
    "    w1 = args[0].w1_\n",
    "\n",
    "    # for euclidean\n",
    "    k2 = args[0].k2_\n",
    "    w2 = args[0].w2_\n",
    "\n",
    "    G1 = args[0].G1_\n",
    "    G2 = args[0].G2_\n",
    "\n",
    "    buckets1 = args[1][0]\n",
    "    buckets2 = args[1][1]\n",
    "\n",
    "    q1 = q[0]\n",
    "    q2 = q[1]\n",
    "\n",
    "\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "\n",
    "    for i in range(L1):\n",
    "        g = G1[i]\n",
    "        g_val = np.array([math.ceil((q1.dot(h[0]) + h[1]) / w1) for h in g])\n",
    "        bi = generate_md5(g_val)\n",
    "\n",
    "        if bi in buckets1[i]:\n",
    "            result1.append(buckets1[i][bi])\n",
    "\n",
    "    for i in range(L2):\n",
    "        g = G2[i]\n",
    "        g_val = np.array([math.ceil((q2.dot(h[0]) + h[1]) / w2) for h in g])\n",
    "        bi = generate_md5(g_val)\n",
    "        \n",
    "        if bi in buckets2[i]:\n",
    "            result2.append(buckets2[i][bi])\n",
    "\n",
    "    if len(result1) == 0:\n",
    "        print('hamming is empty')\n",
    "    \n",
    "    if len(result2) == 0:\n",
    "        print('euclidean is empty')\n",
    "        \n",
    "    result = []\n",
    "    if len(result1) != 0 and len(result2) != 0:\n",
    "        result1 = np.unique(np.concatenate(result1))\n",
    "        result2 = np.unique(np.concatenate(result2))\n",
    "        result = np.unique(np.concatenate([result1, result2]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tle2lsh_param = TLE2LSH(P=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tle2lsh_param.draw_W(c1=3,c2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tle2lsh_param.set_param(100, 200, delta=0.1)\n",
    "tle2lsh_param.generate_auxiliary_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "buckets = TLE2LSH_process(tle2lsh_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for bucket in buckets:\n",
    "    for bu in bucket:\n",
    "        for key, b in bu.items():\n",
    "            size += sys.getsizeof(key)\n",
    "            size += sys.getsizeof(b)\n",
    "size / mbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(metrics.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics(train, TLE2LSH_query, [tle2lsh_param, buckets], \n",
    "        test[:100], dataset_title,\n",
    "        dic_nearest_neighbor[dataset_title], \n",
    "        dic_k_near_neighbors[dataset_title], \n",
    "        verbose=True)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {
    "height": "219px",
    "width": "209px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "745px",
    "left": "1554px",
    "top": "146px",
    "width": "237px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
